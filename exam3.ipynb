{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74b1d10258555fda",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6a3a54e0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Question 2: Explain the role of the regularization parameter C in a Support Vector Machine (SVM) model. How does varying C affect the model’s bias and variance trade-off?\n",
    "\n",
    "In Support Vector Machines (SVMs), the regularisation parameter C is a key hyperparameter that is used to control the complexity and tolerance of the model. The role of C is to balance the trade-off between maximising intervals and minimising classification errors. This balance affects the performance of the SVM model on both training and unseen data.\n",
    "\n",
    "The following is the role and impact of the regularisation parameter C:\n",
    "\n",
    "__Role of C:__\n",
    "C controls the tolerance of the SVM model during training. Smaller values of C encourage greater spacing of the model, i.e., some training samples are allowed to be misclassified in order to maintain the simplicity of the model.\n",
    "Larger values of C cause the model to adapt more tightly to the training data to minimise classification errors, i.e. reduce the spacing, which can lead to more complex decision boundaries.\n",
    "\n",
    "__Affects the bias and variance trade-off of the model:__\n",
    "\n",
    "- __Small C (larger intervals, high bias, low variance):__\n",
    "    - A smaller C value will result in a larger interval for the model, allowing some training samples to be misclassified.\n",
    "    - This will lead to high bias because the model is more concerned with classifying the training data correctly rather than striving for a smaller training error.\n",
    "    - Models with high bias may be oversimplified and insensitive to noise in the data, and therefore may perform better on unseen data.\n",
    "\n",
    "- __Large C (smaller interval, low bias, high variance):__\n",
    "    - A larger C value will result in the model adapting more tightly to the training data to minimise classification errors.\n",
    "    - This will result in low bias as the model is more concerned with classifying the training data correctly, i.e., it seeks a smaller training error.\n",
    "    - Models with low bias may be more complex and sensitive to noise in the training data and therefore may over fit and perform poorly on unseen data.\n",
    "\n",
    "Thus, the choice of C affects the bias and variance trade-off of the SVM model. __Smaller C values produce high bias, low variance models for noisier data, while larger C values produce low bias, high variance models for cleaner data.__ Choosing the appropriate C-value is key, and methods such as cross-validation are often required to determine the optimal hyperparameter settings that will allow the model to perform well on both training and test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839d73fa",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Question 3: Follow the 7-steps to model building for your selected ticker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7f98e89e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca462d89",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Step 1: Data Collection\n",
    "Call the sdk of tushare to grab the daily market data of the stock 600073 from 2010-01-01 to 2022-12-31, including the\n",
    "- Trading Date,\n",
    "- Open Price,\n",
    "- High Price,\n",
    "- Low Price,\n",
    "- Closing Price,\n",
    "- Previous Close Price,\n",
    "- Price Change,\n",
    "- Price Change Percentage,\n",
    "- Volume,\n",
    "- Turnover."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "74c00c3e5528da26",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Stock Code  Trade date  Open Price  High Price  Low Price  Close Price  \\\n",
      "0  600073.SH    20211231        7.95        8.13       7.94         8.09   \n",
      "1  600073.SH    20211230        7.92        7.99       7.92         7.97   \n",
      "2  600073.SH    20211229        8.01        8.05       7.93         7.94   \n",
      "3  600073.SH    20211228        8.07        8.09       8.00         8.03   \n",
      "4  600073.SH    20211227        7.99        8.07       7.96         8.07   \n",
      "\n",
      "   Previous Close Price  Price Change  Price Change Percentage     Volume  \\\n",
      "0                  7.97          0.12                   1.5056  150337.07   \n",
      "1                  7.94          0.03                   0.3778   64586.06   \n",
      "2                  8.03         -0.09                  -1.1208   90722.45   \n",
      "3                  8.07         -0.04                  -0.4957   88413.60   \n",
      "4                  7.99          0.08                   1.0013  123911.38   \n",
      "\n",
      "     Turnover  \n",
      "0  121128.014  \n",
      "1   51445.668  \n",
      "2   72347.819  \n",
      "3   70987.108  \n",
      "4   99343.914  \n"
     ]
    }
   ],
   "source": [
    "# 通过tushare获取某只股票的日行情数据\n",
    "# import tushare as ts\n",
    "#\n",
    "# ts.set_token('7cb6ebc6b67bc4757d18b217c149110ad8f2654766fef3b0a18828ee')\n",
    "# pro = ts.pro_api()\n",
    "# # 上海梅林 600073.SH\n",
    "# df = pro.daily(ts_code='600073.SH', start_date='2010-01-01', end_date='2022-12-31')\n",
    "# print(df)\n",
    "\n",
    "# open csv file\n",
    "# import csv\n",
    "# with open('600073.csv', 'r') as file:\n",
    "#     csv_reader = csv.reader(file)\n",
    "#     for row in csv_reader:\n",
    "#         print(row)\n",
    "\n",
    "# use pandas to read csv\n",
    "data = pd.read_csv('600073.csv')\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f3106d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Step 2: Chose Different Features and Compute Features\n",
    "Use different Features below.\n",
    "1. O-C, means the difference between daily opening and closing prices.\n",
    "2. H-L, means the difference between daily high and low prices.\n",
    "3. Sign, a symbol or momentum used to indicate a price change.\n",
    "4. Past Returns, indicates the price return over a period of time in the past, here I chose the price return of past 5 trading days.\n",
    "5. Momentum, is a characteristic associated with the trend or momentum of price changes and is used to capture rapid changes in stock prices, here I chose the daily change in stock prices.\n",
    "6. SMA, represents the average of prices over a period of time and is used to smooth price data, here I compute the last 20 trading days' average.\n",
    "7. EMA, is a recursive moving average that assigns higher weights to the latest data and is used to track the latest price developments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8ad92f63",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       O-C   H-L  Sign  Past Returns  Momentum      SMA        EMA\n",
      "0     0.14  0.19     1           NaN       NaN      NaN   8.090000\n",
      "1     0.05  0.07     1           NaN     -0.12      NaN   8.078571\n",
      "2    -0.07  0.12     0           NaN     -0.03      NaN   8.065374\n",
      "3    -0.04  0.09     0           NaN      0.09      NaN   8.062005\n",
      "4     0.08  0.11     1           NaN      0.04      NaN   8.062767\n",
      "...    ...   ...   ...           ...       ...      ...        ...\n",
      "2820  0.24  0.26     1     -0.059459     -0.09  10.3635  10.570931\n",
      "2821 -0.30  0.40     0     -0.075540     -0.16  10.3950  10.543223\n",
      "2822 -0.18  0.25     0     -0.035326      0.37  10.4540  10.553392\n",
      "2823  0.12  0.28     1      0.006500      0.19  10.5320  10.580688\n",
      "2824  0.19  0.38     1      0.020893     -0.09  10.6050  10.596813\n",
      "\n",
      "[2825 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "# Calculate new feature values based on the selected features\n",
    "data['O-C'] = data['Close Price'] - data['Open Price']\n",
    "data['H-L'] = data['High Price'] - data['Low Price']\n",
    "# Sign is a labeled column, with 0 indicating a negative trend and 1 indicating a positive trend\n",
    "# Positive ups and downs of less than 0.25% are marked as negative categories\n",
    "data['Sign'] = np.where(data['Price Change Percentage'] > 0.25, 1, 0)\n",
    "data['Past Returns'] = data['Close Price'].pct_change(5)\n",
    "data['Momentum'] = data['Close Price'].diff()\n",
    "data['SMA'] = data['Close Price'].rolling(window=20).mean()\n",
    "Nobs = 20  # where Nobs is the length of the time window\n",
    "alpha = 2 / (Nobs + 1)\n",
    "data['EMA'] = data['Close Price'].ewm(alpha=alpha, adjust=False).mean()\n",
    "\n",
    "features = data[['O-C', 'H-L', 'Sign', 'Past Returns', 'Momentum', 'SMA', 'EMA']]\n",
    "# features = data[['O-C', 'H-L', 'Sign', 'Past Returns']]\n",
    "labels = data['Sign']\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbb9612",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Step 3: Data Preprocessing\n",
    "The main purpose of normalizing the feature data using StandardScaler is to scale the features to a standard normal distribution with mean 0 and standard deviation 1. This process is part of data preprocessing and its main purpose includes:\n",
    "- Feature scaling: different features may have different ranges of values. Normalization allows features to be scaled to similar scales so that the model is easier to handle. This helps to avoid certain features having an excessive impact on the model.\n",
    "- Reduces the risk of model over-fitting: Normalization helps reduce the sensitivity of the model to feature values, reducing the risk of over-fitting.\n",
    "- Improve model performance: certain machine learning algorithms, such as Support Vector Machines and K-Nearest Neighbor, are very sensitive to the scale of the features. Standardization can improve model performance and make it easier to converge.\n",
    "\n",
    "\\`StandardScaler\\` normalizes the value of each feature by subtracting the mean of the feature and dividing by the standard deviation of the feature. This will change the mean of the feature to 0 and the standard deviation to 1.\n",
    "\n",
    "After normalization, \\`scaled_features\\` contains normalized feature data that can be used to build machine learning models. Standardized data are often more suitable for use in many machine learning algorithms because they follow a standard normal distribution, which contributes to model stability and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "48b14f51",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e552a3b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Step 4: Split the Data into Training and Test Sets\n",
    "X_train: this is the array containing the training feature data which will be used to train the machine learning model.\n",
    "\n",
    "X_test: this is the array containing the test feature data, which will be used to evaluate the performance of the model.\n",
    "\n",
    "y_train: this is the array containing the training labels (target values) corresponding to the feature data of the training set. It is used to train the model to learn how to make predictions.\n",
    "\n",
    "y_test: this is the array containing the test labels, corresponding to the feature data of the test set. It is used to evaluate the performance of the model on new data.\n",
    "\n",
    "The main purpose of splitting the dataset into a training set and a test set is to evaluate the generalization ability of the model. By splitting the data into two separate sets, you can test the model's performance on data not seen during training. This helps to determine if the model is able to make accurate predictions on unseen data and to check for overfitting or underfitting problems.\n",
    "\n",
    "In the code, the \\`train_test_split\\` function splits the original dataset of \\`scaled_features\\` and \\`labels\\` into a training set and a test set at a specified ratio (\\`test_size=0.2\\`). the \\`random_state\\` parameter is used to set the random number seed to ensure that the splits are reproducible.\n",
    "\n",
    "Once the dataset is split, we can use the training set to train the model and then use the test set to evaluate the model's performance, such as calculating accuracy, generating confusion matrices, plotting ROC curves, and so on. This helps to determine if the model generalizes enough for use in real-world prediction tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d7ba89ea",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# test_size=0.2 means that 20% of the data will be allocated to the test set, while 80% will be used for the training set\n",
    "# random_state=42 is the seed value used to control the random splitting of the dataset. Specifying the same random_state value will ensure that you get the same random split every time you run the code, making the results repeatable.\n",
    "X_train, X_test, y_train, y_test = train_test_split(scaled_features, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e03c78",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Step 5: Model Construction Random Forest Classifier\n",
    "\\`randomForestClassifier\\` is the class of random forest classifiers used to create a random forest model.\n",
    "\\`n_estimators\\` parameter specifies the number of decision trees to be included in the random forest. Here, a setting of 100 means that the random forest will include 100 decision trees.\n",
    "\\`random_state\\` parameter is used to control the randomness and ensure that the training process of the model is repeatable.\n",
    "\n",
    "The next line of code \\`rf_classifier.fit(X_train, y_train)\\` is used to fit (or train) the model to the training data, where:\n",
    "\\`X_train\\` is the training feature data, containing the features used to train the model.\n",
    "\\`y_train\\` is the training label, i.e., the target variable, corresponding to the training feature data.\n",
    "Through this process, the Random Forest classifier learns how to classify based on the training data. The trained model can be used in subsequent prediction tasks to predict the category labels of new data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "cd311db0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "RandomForestClassifier(random_state=42)",
      "text/html": "<style>#sk-container-id-8 {color: black;}#sk-container-id-8 pre{padding: 0;}#sk-container-id-8 div.sk-toggleable {background-color: white;}#sk-container-id-8 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-8 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-8 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-8 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-8 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-8 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-8 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-8 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-8 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-8 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-8 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-8 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-8 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-8 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-8 div.sk-item {position: relative;z-index: 1;}#sk-container-id-8 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-8 div.sk-item::before, #sk-container-id-8 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-8 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-8 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-8 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-8 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-8 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-8 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-8 div.sk-label-container {text-align: center;}#sk-container-id-8 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-8 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-8\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" checked><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=42)</pre></div></div></div></div></div>"
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# After running this cell, an error occurred.\n",
    "# __ValueError: Input X contains NaN.__\n",
    "# This error indicates that there are missing values (NaN) in my data, and the Random Forest classifier RandomForestClassifier does not handle missing values by default. To solve this problem, I will use data preprocessing techniques to handle the missing values, using \\`SimpleImputer\\` to replace the missing values with the mean values of the corresponding features.\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "X_train_imputed = SimpleImputer(strategy='mean').fit_transform(X_train)\n",
    "X_train = X_train_imputed\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbcaf4a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cffb322",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Step 6: Model Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "55669472",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "y_pred = rf_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03efb9d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Step 6: Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ba18a124",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "#\n",
    "# # 定义要调优的超参数网格\n",
    "# param_grid = {\n",
    "#     'n_estimators': [50, 100, 150],\n",
    "#     'max_depth': [None, 10, 20, 30],\n",
    "#     'min_samples_split': [2, 5, 10]\n",
    "# }\n",
    "#\n",
    "# # 使用交叉验证进行超参数搜索\n",
    "# grid_search = GridSearchCV(rf_classifier, param_grid, cv=5, scoring='accuracy', verbose=2)\n",
    "# grid_search.fit(X_train, y_train)\n",
    "#\n",
    "# # 输出最佳超参数设置\n",
    "# print(\"Best Hyperparameters:\")\n",
    "# print(grid_search.best_params_)\n",
    "#\n",
    "# # 使用最佳超参数重新训练模型\n",
    "# best_rf_classifier = grid_search.best_estimator_\n",
    "# best_rf_classifier.fit(X_train, y_train)\n",
    "#\n",
    "# # 在测试数据上评估性能\n",
    "# y_pred = best_rf_classifier.predict(X_test)\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# roc_auc = roc_auc_score(y_test, y_pred)\n",
    "# conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "# class_report = classification_report(y_test, y_pred)\n",
    "#\n",
    "# print(f\"Accuracy: {accuracy}\")\n",
    "# print(f\"ROC AUC: {roc_auc}\")\n",
    "# print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "# print(f\"Classification Report:\\n{class_report}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beab1aca",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Step 7: Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1b7b01b3",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "ROC AUC: 1.0\n",
      "Confusion Matrix:\n",
      "[[326   0]\n",
      " [  0 239]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       326\n",
      "           1       1.00      1.00      1.00       239\n",
      "\n",
      "    accuracy                           1.00       565\n",
      "   macro avg       1.00      1.00      1.00       565\n",
      "weighted avg       1.00      1.00      1.00       565\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"ROC AUC: {roc_auc}\")\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "print(f\"Classification Report:\\n{class_report}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
